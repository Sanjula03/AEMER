{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ AEMER - Text Emotion Model Training\n",
    "\n",
    "**Architecture:** DistilBERT-based classifier\n",
    "**Dataset:** GoEmotions (Google's Reddit emotion dataset)\n",
    "**Classes:** angry, happy, sad, neutral (mapped from 27 original emotions)\n",
    "**Output:** `text_model.pth` for backend integration\n",
    "\n",
    "---\n",
    "**Instructions:**\n",
    "1. Set Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**\n",
    "2. Run all cells in order\n",
    "3. Download `text_model.pth` when complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load GoEmotions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoEmotions has 27 emotion labels - we'll map to our 4 classes\n",
    "EMOTION_MAP = {\n",
    "    # angry\n",
    "    'anger': 0, 'annoyance': 0, 'disapproval': 0, 'disgust': 0,\n",
    "    # happy\n",
    "    'joy': 1, 'amusement': 1, 'approval': 1, 'excitement': 1, 'gratitude': 1,\n",
    "    'love': 1, 'optimism': 1, 'relief': 1, 'pride': 1, 'admiration': 1,\n",
    "    # sad\n",
    "    'sadness': 2, 'disappointment': 2, 'embarrassment': 2, 'grief': 2,\n",
    "    'remorse': 2, 'fear': 2, 'nervousness': 2,\n",
    "    # neutral\n",
    "    'neutral': 3, 'realization': 3, 'surprise': 3, 'confusion': 3,\n",
    "    'curiosity': 3, 'caring': 3, 'desire': 3\n",
    "}\n",
    "\n",
    "LABEL_NAMES = ['angry', 'happy', 'sad', 'neutral']\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "print(\"Loading GoEmotions dataset...\")\n",
    "dataset = load_dataset('google-research-datasets/go_emotions', 'simplified')\n",
    "print(f\"Train: {len(dataset['train'])} samples\")\n",
    "print(f\"Val: {len(dataset['validation'])} samples\")\n",
    "print(f\"Test: {len(dataset['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get emotion names from dataset\n",
    "emotion_names = dataset['train'].features['labels'].feature.names\n",
    "print(f\"Original emotions: {emotion_names}\")\n",
    "\n",
    "def map_to_4_class(labels):\n",
    "    \"\"\"Map multi-label emotions to single 4-class label.\"\"\"\n",
    "    mapped_labels = []\n",
    "    for label_idx in labels:\n",
    "        emotion = emotion_names[label_idx]\n",
    "        if emotion in EMOTION_MAP:\n",
    "            mapped_labels.append(EMOTION_MAP[emotion])\n",
    "    \n",
    "    if not mapped_labels:\n",
    "        return 3  # neutral if no mapping\n",
    "    \n",
    "    # Priority-based selection: angry > sad > happy > neutral\n",
    "    if 0 in mapped_labels: return 0  # angry\n",
    "    if 2 in mapped_labels: return 2  # sad\n",
    "    if 1 in mapped_labels: return 1  # happy\n",
    "    return 3  # neutral\n",
    "\n",
    "# Process datasets\n",
    "def process_split(split):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for item in split:\n",
    "        texts.append(item['text'])\n",
    "        labels.append(map_to_4_class(item['labels']))\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = process_split(dataset['train'])\n",
    "val_texts, val_labels = process_split(dataset['validation'])\n",
    "test_texts, test_labels = process_split(dataset['test'])\n",
    "\n",
    "print(f\"\\n\\U0001f4ca Class distribution (train):\")\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    count = train_labels.count(i)\n",
    "    print(f\"  {name}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
    "print(f\"  Total: {len(train_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Tokenizer & Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "MAX_LEN = 128\n",
    "\n",
    "class TextEmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Use __call__ method (compatible with all transformers versions)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextEmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = TextEmotionDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "test_dataset = TextEmotionDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"‚úÖ Datasets created\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Val: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss helps with class imbalance by down-weighting the loss assigned \n",
    "    to well-classified examples. It explicitly forces the model to focus on\n",
    "    hard, misclassified examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "class TextEmotionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    DistilBERT-based text emotion classifier.\n",
    "    Freezes early layers, uses a deeper classifier head.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Freeze embeddings + first 4 of 6 transformer layers\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        for i, layer in enumerate(self.bert.transformer.layer):\n",
    "            if i < 4:  # Freeze layers 0-3, layers 4-5 are trainable\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Deeper classifier head to learn complex features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(384, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        return self.classifier(x)\n",
    "\n",
    "model = TextEmotionClassifier(num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\u2705 Model created\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "print(f\"  Frozen: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE = 3\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Calculate strong class weights for the imbalanced dataset\n",
    "class_counts = np.array([train_labels.count(i) for i in range(NUM_CLASSES)])\n",
    "class_weights = 1.0 / np.log1p(class_counts)  # Log smoothing for weights\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES\n",
    "\n",
    "# USE FOCAL LOSS: Aggressively penalizes easy examples (Neutral)\n",
    "criterion = FocalLoss(weight=class_weights, gamma=2.0)\n",
    "\n",
    "# Differential learning rates\n",
    "bert_params = [p for n, p in model.named_parameters() if 'bert' in n and p.requires_grad]\n",
    "classifier_params = [p for n, p in model.named_parameters() if 'bert' not in n and p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': bert_params, 'lr': 2e-5},\n",
    "    {'params': classifier_params, 'lr': 1e-4},\n",
    "], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Warmup + linear decay\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(f\"\\U0001f4ca Training config:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  BERT LR: 2e-5 | Classifier LR: 1e-4\")\n",
    "print(f\"  Loss Function: Focal Loss (gamma=2.0)\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"  Warmup steps: {warmup_steps}\")\n",
    "print(f\"  Class weights: {class_weights.cpu().numpy().round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Step per batch for warmup+decay\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with warmup schedule and early stopping\n",
    "print(\"\\U0001f680 Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_loss = float('inf')  # Track val_loss instead of val_acc\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping based on val_loss (lower is better)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'text_model.pth')\n",
    "        print(f\"  \\u2705 Best model saved! (val_loss: {val_loss:.4f}, acc: {val_acc:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  \\u26a0\\ufe0f No improvement ({patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n\\U0001f6d1 Early stopping triggered at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\u2705 Training complete!\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best val acc: {best_val_acc:.4f}\")\n",
    "print(f\"  Total epochs run: {len(history['train_loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('text_model.pth'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nüìä Test Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(\"\\n\" + classification_report(all_labels, all_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Val')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('text_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Confusion Matrix (shows recall per class)\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize by row\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Normalized (percentages)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[0],\n",
    "            vmin=0, vmax=1)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Normalized Confusion Matrix (Recall per Class)')\n",
    "\n",
    "# Right: Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Raw Counts)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('text_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print per-class accuracy\n",
    "print(\"\\nPer-class accuracy (recall):\")\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    recall = cm_normalized[i, i]\n",
    "    print(f\"  {name}: {recall:.1%}\")\n",
    "print(f\"  Overall: {np.trace(cm)/cm.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Test with Custom Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text, model, tokenizer, device):\n",
    "    \"\"\"Predict emotion from text.\"\"\"\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "        conf = probs[0][pred].item()\n",
    "    \n",
    "    return LABEL_NAMES[pred], conf, {LABEL_NAMES[i]: probs[0][i].item() for i in range(4)}\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"I'm so happy today! Everything is going great!\",\n",
    "    \"This is absolutely terrible, I'm furious!\",\n",
    "    \"I feel so lonely and depressed...\",\n",
    "    \"The meeting is scheduled for 3 PM.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Testing with sample texts:\")\n",
    "print(\"=\" * 60)\n",
    "for text in test_texts:\n",
    "    emotion, conf, probs = predict_emotion(text, model, tokenizer, device)\n",
    "    print(f\"\\nText: \\\"{text[:50]}...\\\"\" if len(text) > 50 else f\"\\nText: \\\"{text}\\\"\")\n",
    "    print(f\"  ‚Üí {emotion.upper()} ({conf:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Save & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model with metadata\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'max_len': MAX_LEN,\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'test_acc': test_acc\n",
    "}, 'text_model_full.pth')\n",
    "\n",
    "print(\"‚úÖ Files saved:\")\n",
    "print(\"  üìÅ text_model.pth - State dict only (for inference)\")\n",
    "print(\"  üìÅ text_model_full.pth - With metadata\")\n",
    "print(\"  üìÅ text_training_curves.png\")\n",
    "print(\"  üìÅ text_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"üì• Click to download:\")\n",
    "    files.download('text_model.pth')\n",
    "    files.download('text_model_full.pth')\n",
    "except:\n",
    "    print(\"Files saved - manually download from file browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "**Next steps:**\n",
    "1. Download `text_model.pth`\n",
    "2. Create `TextModel/` folder in your project\n",
    "3. Put the model file there\n",
    "4. The backend will integrate it with emotion recognition"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}