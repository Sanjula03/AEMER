{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ AEMER - Accent Detection Model Training (11+ Accents)\n",
    "\n",
    "**Architecture:** CNN-BiLSTM with Attention\n",
    "\n",
    "**Dataset:** Kaggle Speech Accent Archive (2,138 real recordings)\n",
    "\n",
    "**Author:** Sanjula Sunath | w1999522\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install torch torchaudio librosa pandas numpy scikit-learn tqdm matplotlib seaborn kagglehub --quiet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 9 Real Accent Classes ===\n",
    "ACCENT_LABELS = {\n",
    "    0: 'american',     # üá∫üá∏ United States\n",
    "    1: 'british',      # üá¨üáß England\n",
    "    2: 'australian',   # üá¶üá∫ Australia\n",
    "    3: 'indian',       # üáÆüá≥ South Asia\n",
    "    4: 'canadian',     # üá®üá¶ Canada\n",
    "    5: 'irish',        # üáÆüá™ Ireland\n",
    "    6: 'african',      # üåç Africa\n",
    "    7: 'filipino',     # üáµüá≠ Philippines\n",
    "    8: 'hongkong',     # üá≠üá∞ Hong Kong / East Asia\n",
    "}\n",
    "ACCENT_TO_IDX = {v: k for k, v in ACCENT_LABELS.items()}\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 160\n",
    "MAX_LEN = int(SAMPLE_RATE * DURATION / HOP_LENGTH) + 1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"üéØ {NUM_CLASSES} accent classes, {DURATION}s clips, {EPOCHS} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download Speech Accent Archive\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import kagglehub\n",
    "\n",
    "print(\"üì• Downloading Speech Accent Archive from Kaggle...\")\n",
    "dataset_path = kagglehub.dataset_download(\"rtatman/speech-accent-archive\")\n",
    "print(f\"‚úÖ Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Find recordings directory (nested: recordings/recordings/)\n",
    "recordings_dir = os.path.join(dataset_path, 'recordings', 'recordings')\n",
    "if not os.path.exists(recordings_dir):\n",
    "    # Fallback: search for MP3 files\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        mp3s = [f for f in files if f.endswith('.mp3')]\n",
    "        if mp3s:\n",
    "            recordings_dir = root\n",
    "            break\n",
    "\n",
    "audio_files = [f for f in os.listdir(recordings_dir) if f.endswith('.mp3')]\n",
    "print(f\"üìÇ Recordings: {recordings_dir}\")\n",
    "print(f\"   Found {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build Accent Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load metadata CSV\n",
    "csv_files = glob.glob(os.path.join(dataset_path, '**', '*.csv'), recursive=True)\n",
    "df = pd.read_csv(csv_files[0])\n",
    "print(f\"Metadata: {df.shape[0]} rows\")\n",
    "print(f\"\\nTop 20 native languages:\")\n",
    "print(df['native_language'].value_counts().head(20))\n",
    "\n",
    "# Map native languages ‚Üí 9 REAL accent classes only\n",
    "LANGUAGE_TO_ACCENT = {\n",
    "    'hindi': 'indian', 'urdu': 'indian', 'bengali': 'indian',\n",
    "    'tamil': 'indian', 'telugu': 'indian', 'gujarati': 'indian',\n",
    "    'punjabi': 'indian', 'marathi': 'indian', 'kannada': 'indian',\n",
    "    'malayalam': 'indian', 'nepali': 'indian', 'sinhala': 'indian',\n",
    "    'sinhalese': 'indian', 'dari': 'indian',\n",
    "    'mandarin': 'hongkong', 'cantonese': 'hongkong',\n",
    "    'chinese': 'hongkong', 'japanese': 'hongkong', 'korean': 'hongkong',\n",
    "    'tagalog': 'filipino', 'cebuano': 'filipino', 'ilocano': 'filipino',\n",
    "    'amharic': 'african', 'swahili': 'african', 'yoruba': 'african',\n",
    "    'igbo': 'african', 'hausa': 'african', 'zulu': 'african',\n",
    "    'twi': 'african', 'shona': 'african', 'akan': 'african',\n",
    "    'luganda': 'african', 'wolof': 'african', 'somali': 'african',\n",
    "    'kinyarwanda': 'african', 'xhosa': 'african', 'afrikaans': 'african',\n",
    "    'arabic': 'african',\n",
    "    'irish': 'irish', 'gaelic': 'irish',\n",
    "}\n",
    "\n",
    "COUNTRY_TO_ACCENT = {\n",
    "    'usa': 'american', 'united states': 'american',\n",
    "    'uk': 'british', 'england': 'british', 'united kingdom': 'british',\n",
    "    'australia': 'australian',\n",
    "    'canada': 'canadian',\n",
    "    'scotland': 'british',\n",
    "    'ireland': 'irish',\n",
    "    'south africa': 'african', 'nigeria': 'african', 'kenya': 'african', 'ghana': 'african',\n",
    "    'india': 'indian', 'pakistan': 'indian', 'bangladesh': 'indian', 'sri lanka': 'indian',\n",
    "    'philippines': 'filipino',\n",
    "    'hong kong': 'hongkong',\n",
    "}\n",
    "\n",
    "data_list = []\n",
    "accent_counts = Counter()\n",
    "skipped = 0\n",
    "\n",
    "print(\"\\nüîÑ Processing speakers...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "    try:\n",
    "        lang = str(row.get('native_language', '')).strip().lower()\n",
    "        country = str(row.get('country', '')).strip().lower()\n",
    "        birth_place = str(row.get('birthplace', '')).strip().lower()\n",
    "        filename = str(row.get('filename', ''))\n",
    "\n",
    "        if not filename or filename == 'nan':\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        accent = None\n",
    "        if lang in LANGUAGE_TO_ACCENT:\n",
    "            accent = LANGUAGE_TO_ACCENT[lang]\n",
    "        if accent is None and lang == 'english':\n",
    "            for key, acc in COUNTRY_TO_ACCENT.items():\n",
    "                if key in country or key in birth_place:\n",
    "                    accent = acc\n",
    "                    break\n",
    "            if accent is None:\n",
    "                accent = 'american'\n",
    "        if accent is None:\n",
    "            for key, acc in COUNTRY_TO_ACCENT.items():\n",
    "                if key in country or key in birth_place:\n",
    "                    accent = acc\n",
    "                    break\n",
    "\n",
    "        if accent is None or accent not in ACCENT_TO_IDX:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        audio_path = None\n",
    "        for ext in ['.mp3', '.wav', '']:\n",
    "            candidate = os.path.join(recordings_dir, filename + ext)\n",
    "            if os.path.exists(candidate):\n",
    "                audio_path = candidate\n",
    "                break\n",
    "            candidate = os.path.join(recordings_dir, filename)\n",
    "            if os.path.exists(candidate):\n",
    "                audio_path = candidate\n",
    "                break\n",
    "\n",
    "        if audio_path is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        data_list.append({\n",
    "            'audio_path': audio_path,\n",
    "            'label': ACCENT_TO_IDX[accent],\n",
    "            'accent': accent,\n",
    "        })\n",
    "        accent_counts[accent] += 1\n",
    "    except:\n",
    "        skipped += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Processed: {len(data_list)} samples (skipped: {skipped})\")\n",
    "print(f\"\\nüìä Samples per accent:\")\n",
    "for accent, count in sorted(accent_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {accent}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Balance Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Filter accents with enough samples and balance\n",
    "MIN_SAMPLES = 10\n",
    "samples_by_class = defaultdict(list)\n",
    "for d in data_list:\n",
    "    samples_by_class[d['label']].append(d)\n",
    "\n",
    "valid_classes = {label for label, samples in samples_by_class.items() if len(samples) >= MIN_SAMPLES}\n",
    "\n",
    "print(f\"Accents with >= {MIN_SAMPLES} samples:\")\n",
    "for label in sorted(valid_classes):\n",
    "    print(f\"  [{label}] {ACCENT_LABELS[label]}: {len(samples_by_class[label])}\")\n",
    "\n",
    "# Remap labels to be contiguous\n",
    "valid_data = [d for d in data_list if d['label'] in valid_classes]\n",
    "sorted_valid = sorted(valid_classes)\n",
    "old_to_new = {old: new for new, old in enumerate(sorted_valid)}\n",
    "\n",
    "FINAL_ACCENTS = [ACCENT_LABELS[old] for old in sorted_valid]\n",
    "FINAL_ACCENT_TO_IDX = {a: i for i, a in enumerate(FINAL_ACCENTS)}\n",
    "FINAL_IDX_TO_ACCENT = {i: a for i, a in enumerate(FINAL_ACCENTS)}\n",
    "FINAL_NUM_CLASSES = len(FINAL_ACCENTS)\n",
    "\n",
    "for d in valid_data:\n",
    "    d['label'] = old_to_new[d['label']]\n",
    "\n",
    "# Balance with oversampling\n",
    "target_size = int(np.median([len(samples_by_class[c]) for c in valid_classes]))\n",
    "target_size = max(target_size, MIN_SAMPLES * 2)\n",
    "\n",
    "balanced_data = []\n",
    "new_by_class = defaultdict(list)\n",
    "for d in valid_data:\n",
    "    new_by_class[d['label']].append(d)\n",
    "\n",
    "for label, samples in new_by_class.items():\n",
    "    if len(samples) >= target_size:\n",
    "        balanced_data.extend(random.sample(samples, target_size))\n",
    "    else:\n",
    "        balanced_data.extend(samples)\n",
    "        balanced_data.extend(random.choices(samples, k=target_size - len(samples)))\n",
    "\n",
    "random.shuffle(balanced_data)\n",
    "data_list = balanced_data\n",
    "\n",
    "print(f\"\\n‚úÖ Final: {FINAL_NUM_CLASSES} accent classes, {len(data_list)} total samples\")\n",
    "print(f\"\\nüìä Balanced distribution:\")\n",
    "for label in sorted(set(d['label'] for d in data_list)):\n",
    "    count = sum(1 for d in data_list if d['label'] == label)\n",
    "    print(f\"  [{label}] {FINAL_IDX_TO_ACCENT[label]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, data_list, augment=False):\n",
    "        self.data = data_list\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_audio(self, audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=DURATION + 1)\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "\n",
    "        target = int(DURATION * SAMPLE_RATE)\n",
    "        if len(audio) < target:\n",
    "            audio = np.pad(audio, (0, target - len(audio)))\n",
    "        else:\n",
    "            if self.augment:\n",
    "                start = random.randint(0, max(0, len(audio) - target))\n",
    "                audio = audio[start:start + target]\n",
    "            else:\n",
    "                audio = audio[:target]\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.3:\n",
    "                audio = audio + np.random.randn(len(audio)) * 0.005\n",
    "            if random.random() < 0.2:\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=SAMPLE_RATE, n_steps=random.uniform(-1, 1))\n",
    "            if random.random() < 0.2:\n",
    "                audio = librosa.effects.time_stretch(audio, rate=random.uniform(0.9, 1.1))\n",
    "                if len(audio) < target:\n",
    "                    audio = np.pad(audio, (0, target - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target]\n",
    "\n",
    "        mel = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        spec = librosa.power_to_db(mel, ref=np.max)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-8)\n",
    "        if spec.shape[1] < MAX_LEN:\n",
    "            spec = np.pad(spec, ((0, 0), (0, MAX_LEN - spec.shape[1])))\n",
    "        else:\n",
    "            spec = spec[:, :MAX_LEN]\n",
    "        return spec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        try:\n",
    "            spec = self.process_audio(item['audio_path'])\n",
    "        except:\n",
    "            spec = np.zeros((N_MELS, MAX_LEN))\n",
    "        return torch.FloatTensor(spec).unsqueeze(0), item['label']\n",
    "\n",
    "print(\"‚úÖ AccentDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ CNN-BiLSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNN_BiLSTM_Accent(nn.Module):\n",
    "    def __init__(self, num_classes=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.3))\n",
    "        self.lstm = nn.LSTM(128 * 16, 128, 2, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.attention = nn.Sequential(nn.Linear(256, 64), nn.Tanh(), nn.Linear(64, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.conv2(self.conv1(x)))\n",
    "        x = x.permute(0, 3, 1, 2).reshape(x.size(0), -1, 128 * 16)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        return self.fc(torch.sum(attn * lstm_out, dim=1))\n",
    "\n",
    "model = CNN_BiLSTM_Accent(FINAL_NUM_CLASSES).to(device)\n",
    "print(f\"‚úÖ Model: {FINAL_NUM_CLASSES} classes, {sum(p.numel() for p in model.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Data Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data, val_data = train_test_split(\n",
    "    data_list, test_size=0.2, random_state=42,\n",
    "    stratify=[d['label'] for d in data_list]\n",
    ")\n",
    "\n",
    "train_dataset = AccentDataset(train_data, augment=True)\n",
    "val_dataset = AccentDataset(val_data, augment=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(train_data)} ({len(train_loader)} batches)\")\n",
    "print(f\"Val: {len(val_data)} ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "label_counts = Counter(d['label'] for d in train_data)\n",
    "total = sum(label_counts.values())\n",
    "class_weights = torch.FloatTensor([total / (FINAL_NUM_CLASSES * label_counts.get(i, 1)) for i in range(FINAL_NUM_CLASSES)]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "PATIENCE = 12\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(f\"Training for up to {EPOCHS} epochs (patience={PATIENCE})...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss, correct, total_n = 0, 0, 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        total_n += labels.size(0)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "    train_acc = correct / total_n\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, correct, total_n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, pred = outputs.max(1)\n",
    "            total_n += labels.size(0)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "    val_acc = correct / total_n\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history['train_loss'].append(train_loss / len(train_loader))\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss / len(val_loader))\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'accent_model.pth')\n",
    "        print(f\"Epoch {epoch+1}: Train {train_acc:.3f} | Val {val_acc:.3f} ‚≠ê BEST\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Epoch {epoch+1}: Train {train_acc:.3f} | Val {val_acc:.3f} ({patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nüõë Early stopping at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Val Acc: {best_val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Training Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(history['train_loss'], label='Train'); axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss'); axes[0].legend(); axes[0].grid(True)\n",
    "axes[1].plot([a*100 for a in history['train_acc']], label='Train')\n",
    "axes[1].plot([a*100 for a in history['val_acc']], label='Val')\n",
    "axes[1].set_title('Accuracy (%)'); axes[1].legend(); axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('accent_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.load_state_dict(torch.load('accent_model.pth'))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs.to(device))\n",
    "        _, pred = outputs.max(1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accent_names = [FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)]\n",
    "\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=accent_names))\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.1%', cmap='Blues',\n",
    "            xticklabels=accent_names, yticklabels=accent_names, ax=axes[0], vmin=0, vmax=1)\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Normalized (Recall)'); axes[0].tick_params(axis='x', rotation=45)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=accent_names, yticklabels=accent_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Raw Counts'); axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('accent_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Per-class recall:\")\n",
    "for i, name in enumerate(accent_names):\n",
    "    print(f\"  {name}: {cm_norm[i, i]:.1%}\")\n",
    "print(f\"  Overall: {np.trace(cm)/cm.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Save & Download\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save with metadata\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'accent_labels': FINAL_IDX_TO_ACCENT,\n",
    "    'num_classes': FINAL_NUM_CLASSES,\n",
    "    'accents_list': FINAL_ACCENTS,\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'n_mels': N_MELS,\n",
    "    'n_fft': N_FFT,\n",
    "    'hop_length': HOP_LENGTH,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'accent_model_full.pth')\n",
    "\n",
    "print(f\"‚úÖ Saved accent_model.pth\")\n",
    "print(f\"\\nüéØ Final {FINAL_NUM_CLASSES} accent classes:\")\n",
    "for i in range(FINAL_NUM_CLASSES):\n",
    "    print(f\"  [{i}] {FINAL_IDX_TO_ACCENT[i]}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è COPY THIS TO app.py:\")\n",
    "print(f\"ACCENTS = {FINAL_ACCENTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import files\n",
    "files.download('accent_model.pth')\n",
    "files.download('accent_model_full.pth')\n",
    "files.download('accent_training_curves.png')\n",
    "files.download('accent_confusion_matrix.png')"
   ]
  }
 ]
}