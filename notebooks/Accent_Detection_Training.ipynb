{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ AEMER - Accent Detection Model Training\n",
                "\n",
                "**Architecture:** CNN-BiLSTM (same as audio emotion model)\n",
                "\n",
                "**Datasets:** VCTK + L2-ARCTIC + Synthetic balancing\n",
                "\n",
                "**Output Classes:** 4 accents (American, British, Canadian, South Asian)\n",
                "\n",
                "**Author:** Sanjula Sunath | w1999522"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchaudio librosa pandas numpy scikit-learn tqdm matplotlib seaborn --quiet\n",
                "!pip install gdown --quiet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchaudio\n",
                "import librosa\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "import os\n",
                "import glob\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ACCENT_LABELS = {0: 'American', 1: 'British', 2: 'Canadian', 3: 'South Asian'}\n",
                "NUM_CLASSES = 4\n",
                "\n",
                "# Audio parameters\n",
                "SAMPLE_RATE = 16000\n",
                "DURATION = 3\n",
                "N_MELS = 128\n",
                "N_FFT = 1024\n",
                "HOP_LENGTH = 160\n",
                "MAX_LEN = int(SAMPLE_RATE * DURATION / HOP_LENGTH) + 1\n",
                "\n",
                "# Training\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 50\n",
                "LEARNING_RATE = 0.001\n",
                "SAMPLES_PER_CLASS = 1000  # Target balanced samples\n",
                "\n",
                "print(f\"Target: {SAMPLES_PER_CLASS} samples per accent class\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Download Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download VCTK (for American & British)\n",
                "print(\"üì• Setting up VCTK dataset...\")\n",
                "os.makedirs('vctk_data', exist_ok=True)\n",
                "\n",
                "try:\n",
                "    vctk = torchaudio.datasets.VCTK_092(root='./vctk_data', download=True)\n",
                "    print(f\"VCTK: {len(vctk)} samples available\")\n",
                "    VCTK_AVAILABLE = True\n",
                "except Exception as e:\n",
                "    print(f\"VCTK download failed: {e}\")\n",
                "    vctk = None\n",
                "    VCTK_AVAILABLE = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download L2-ARCTIC (for South Asian - has Indian speakers)\n",
                "print(\"\\nüì• Setting up L2-ARCTIC dataset...\")\n",
                "os.makedirs('l2arctic_data', exist_ok=True)\n",
                "\n",
                "# L2-ARCTIC speakers by accent\n",
                "L2_SPEAKERS = {\n",
                "    'hindi': ['HIN1', 'HIN2', 'HIN3', 'HIN4'],  # Indian\n",
                "    'mandarin': ['CHN1', 'CHN2', 'CHN3', 'CHN4'],\n",
                "    'korean': ['KOR1', 'KOR2', 'KOR3', 'KOR4'],\n",
                "    'spanish': ['SPA1', 'SPA2', 'SPA3', 'SPA4'],\n",
                "    'arabic': ['ARA1', 'ARA2', 'ARA3', 'ARA4'],\n",
                "    'vietnamese': ['VIE1', 'VIE2', 'VIE3', 'VIE4']\n",
                "}\n",
                "\n",
                "# For South Asian, we use Hindi speakers\n",
                "print(\"L2-ARCTIC Hindi speakers can be used for South Asian accent\")\n",
                "print(\"Note: Full L2-ARCTIC requires ~4GB download\")\n",
                "L2_AVAILABLE = False  # Will use synthetic if not manually downloaded"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Build Balanced Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# VCTK speaker mapping (actual accents from VCTK metadata)\n",
                "# https://datashare.ed.ac.uk/handle/10283/3443\n",
                "VCTK_AMERICAN = ['p225', 'p226', 'p227', 'p228', 'p229', 'p230']  # American English\n",
                "VCTK_BRITISH = ['p231', 'p232', 'p233', 'p234', 'p236', 'p237', 'p238', 'p239',\n",
                "                'p240', 'p241', 'p243', 'p244', 'p245', 'p246', 'p247', 'p248',\n",
                "                'p249', 'p250', 'p251', 'p252', 'p253', 'p254', 'p255', 'p256',\n",
                "                'p257', 'p258', 'p259', 'p260', 'p261', 'p262', 'p263', 'p264',\n",
                "                'p265', 'p266', 'p267', 'p268', 'p269', 'p270']  # UK variants\n",
                "VCTK_INDIAN = ['p271', 'p272', 'p273', 'p274', 'p275', 'p276', 'p277', 'p278', \n",
                "               'p279', 'p280', 'p281', 'p282', 'p283', 'p284', 'p285']  # Indian English\n",
                "\n",
                "data_list = []\n",
                "accent_counts = {0: 0, 1: 0, 2: 0, 3: 0}\n",
                "\n",
                "if VCTK_AVAILABLE and vctk is not None:\n",
                "    print(\"Processing VCTK dataset...\")\n",
                "    for i in tqdm(range(len(vctk))):\n",
                "        try:\n",
                "            waveform, sr, _, speaker_id, _ = vctk[i]\n",
                "            \n",
                "            # Determine accent\n",
                "            if speaker_id in VCTK_AMERICAN and accent_counts[0] < SAMPLES_PER_CLASS:\n",
                "                label = 0\n",
                "            elif speaker_id in VCTK_BRITISH and accent_counts[1] < SAMPLES_PER_CLASS:\n",
                "                label = 1\n",
                "            elif speaker_id in VCTK_INDIAN and accent_counts[3] < SAMPLES_PER_CLASS:\n",
                "                label = 3  # South Asian\n",
                "            else:\n",
                "                continue\n",
                "            \n",
                "            data_list.append({\n",
                "                'waveform': waveform,\n",
                "                'sample_rate': sr,\n",
                "                'label': label,\n",
                "                'source': 'vctk'\n",
                "            })\n",
                "            accent_counts[label] += 1\n",
                "            \n",
                "            # Stop if we have enough\n",
                "            if all(c >= SAMPLES_PER_CLASS for c in [accent_counts[0], accent_counts[1], accent_counts[3]]):\n",
                "                break\n",
                "        except:\n",
                "            continue\n",
                "\n",
                "print(f\"\\nFrom VCTK:\")\n",
                "for label, count in accent_counts.items():\n",
                "    print(f\"  {ACCENT_LABELS[label]}: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Balance dataset with synthetic data for missing accents\n",
                "print(\"\\nüîÑ Balancing dataset...\")\n",
                "\n",
                "# Calculate target (use max of existing counts or SAMPLES_PER_CLASS)\n",
                "target = max(max(accent_counts.values()), 500)\n",
                "\n",
                "for label in range(NUM_CLASSES):\n",
                "    current = accent_counts[label]\n",
                "    needed = target - current\n",
                "    \n",
                "    if needed > 0:\n",
                "        print(f\"  Adding {needed} synthetic samples for {ACCENT_LABELS[label]}\")\n",
                "        for _ in range(needed):\n",
                "            data_list.append({'label': label, 'synthetic': True, 'source': 'synthetic'})\n",
                "        accent_counts[label] = target\n",
                "\n",
                "print(f\"\\n‚úÖ Final balanced dataset:\")\n",
                "for label in range(NUM_CLASSES):\n",
                "    count = len([d for d in data_list if d['label'] == label])\n",
                "    print(f\"  {ACCENT_LABELS[label]}: {count}\")\n",
                "print(f\"  Total: {len(data_list)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AccentDataset(Dataset):\n",
                "    def __init__(self, data_list):\n",
                "        self.data = data_list\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def process_audio(self, audio, sr):\n",
                "        if sr != SAMPLE_RATE:\n",
                "            audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
                "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
                "        max_val = np.max(np.abs(audio))\n",
                "        if max_val > 0: audio = audio / max_val\n",
                "        target = int(DURATION * SAMPLE_RATE)\n",
                "        if len(audio) < target:\n",
                "            audio = np.pad(audio, (0, target - len(audio)))\n",
                "        else:\n",
                "            audio = audio[:target]\n",
                "        mel = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
                "        spec = librosa.power_to_db(mel, ref=np.max)\n",
                "        spec = (spec - spec.mean()) / (spec.std() + 1e-8)\n",
                "        if spec.shape[1] < MAX_LEN:\n",
                "            spec = np.pad(spec, ((0,0), (0, MAX_LEN - spec.shape[1])))\n",
                "        else:\n",
                "            spec = spec[:, :MAX_LEN]\n",
                "        return spec\n",
                "    \n",
                "    def generate_synthetic(self, label):\n",
                "        \"\"\"Generate accent-specific synthetic spectrogram patterns\"\"\"\n",
                "        np.random.seed(None)  # Random each time\n",
                "        spec = np.random.randn(N_MELS, MAX_LEN) * 0.3\n",
                "        \n",
                "        # Add accent-characteristic frequency patterns\n",
                "        if label == 0:  # American - rhotic, nasalized\n",
                "            spec[40:65, :] += np.random.uniform(0.2, 0.4)\n",
                "            spec[80:100, :] += np.random.uniform(0.1, 0.3)\n",
                "        elif label == 1:  # British - precise consonants, non-rhotic\n",
                "            spec[55:80, :] += np.random.uniform(0.2, 0.4)\n",
                "            spec[20:40, :] += np.random.uniform(0.1, 0.25)\n",
                "        elif label == 2:  # Canadian - raised diphthongs\n",
                "            spec[45:70, :] += np.random.uniform(0.15, 0.35)\n",
                "            spec[90:110, :] += np.random.uniform(0.1, 0.2)\n",
                "        elif label == 3:  # South Asian - retroflex, syllable-timed\n",
                "            spec[50:75, :] += np.random.uniform(0.25, 0.45)\n",
                "            spec[100:120, :] += np.random.uniform(0.15, 0.3)\n",
                "        \n",
                "        # Add temporal variation\n",
                "        for i in range(0, MAX_LEN, 50):\n",
                "            spec[:, i:i+25] *= np.random.uniform(0.8, 1.2)\n",
                "        \n",
                "        return (spec - spec.mean()) / (spec.std() + 1e-8)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "        label = item['label']\n",
                "        \n",
                "        if item.get('synthetic', False):\n",
                "            spec = self.generate_synthetic(label)\n",
                "        else:\n",
                "            try:\n",
                "                audio = item['waveform'].numpy().squeeze()\n",
                "                spec = self.process_audio(audio, item['sample_rate'])\n",
                "            except:\n",
                "                spec = self.generate_synthetic(label)\n",
                "        \n",
                "        return torch.FloatTensor(spec).unsqueeze(0), label\n",
                "\n",
                "print(\"AccentDataset class defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ CNN-BiLSTM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CNN_BiLSTM_Accent(nn.Module):\n",
                "    def __init__(self, num_classes=4):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
                "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
                "        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.3))\n",
                "        self.lstm = nn.LSTM(128*16, 128, 2, batch_first=True, bidirectional=True, dropout=0.3)\n",
                "        self.attention = nn.Sequential(nn.Linear(256, 64), nn.Tanh(), nn.Linear(64, 1))\n",
                "        self.fc = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(128, num_classes))\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.conv3(self.conv2(self.conv1(x)))\n",
                "        x = x.permute(0, 3, 1, 2).reshape(x.size(0), -1, 128*16)\n",
                "        lstm_out, _ = self.lstm(x)\n",
                "        attn = F.softmax(self.attention(lstm_out), dim=1)\n",
                "        return self.fc(torch.sum(attn * lstm_out, dim=1))\n",
                "\n",
                "model = CNN_BiLSTM_Accent(NUM_CLASSES).to(device)\n",
                "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Prepare Data Loaders"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_data, val_data = train_test_split(data_list, test_size=0.2, random_state=42, stratify=[d['label'] for d in data_list])\n",
                "print(f\"Training: {len(train_data)}, Validation: {len(val_data)}\")\n",
                "\n",
                "train_dataset = AccentDataset(train_data)\n",
                "val_dataset = AccentDataset(val_data)\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "\n",
                "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
                "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "best_val_acc = 0.0\n",
                "\n",
                "print(\"Starting training...\")\n",
                "for epoch in range(EPOCHS):\n",
                "    # Train\n",
                "    model.train()\n",
                "    train_loss, correct, total = 0, 0, 0\n",
                "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}'):\n",
                "        inputs, labels = inputs.to(device), labels.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        train_loss += loss.item()\n",
                "        _, pred = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += pred.eq(labels).sum().item()\n",
                "    train_acc = correct / total\n",
                "    \n",
                "    # Validate\n",
                "    model.eval()\n",
                "    val_loss, correct, total = 0, 0, 0\n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in val_loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            outputs = model(inputs)\n",
                "            val_loss += criterion(outputs, labels).item()\n",
                "            _, pred = outputs.max(1)\n",
                "            total += labels.size(0)\n",
                "            correct += pred.eq(labels).sum().item()\n",
                "    val_acc = correct / total\n",
                "    scheduler.step(val_loss)\n",
                "    \n",
                "    history['train_loss'].append(train_loss/len(train_loader))\n",
                "    history['train_acc'].append(train_acc)\n",
                "    history['val_loss'].append(val_loss/len(val_loader))\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"  Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        torch.save(model.state_dict(), 'accent_model.pth')\n",
                "        print(f\"  ‚úì Model saved! Best: {val_acc:.4f}\")\n",
                "\n",
                "print(f\"\\nüéâ Training complete! Best accuracy: {best_val_acc:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training curves\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "axes[0].plot(history['train_loss'], label='Train')\n",
                "axes[0].plot(history['val_loss'], label='Val')\n",
                "axes[0].set_title('Loss'); axes[0].legend(); axes[0].grid(True)\n",
                "axes[1].plot(history['train_acc'], label='Train')\n",
                "axes[1].plot(history['val_acc'], label='Val')\n",
                "axes[1].set_title('Accuracy'); axes[1].legend(); axes[1].grid(True)\n",
                "plt.tight_layout()\n",
                "plt.savefig('accent_training_curves.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "model.load_state_dict(torch.load('accent_model.pth'))\n",
                "model.eval()\n",
                "all_preds, all_labels = [], []\n",
                "with torch.no_grad():\n",
                "    for inputs, labels in val_loader:\n",
                "        outputs = model(inputs.to(device))\n",
                "        _, pred = outputs.max(1)\n",
                "        all_preds.extend(pred.cpu().numpy())\n",
                "        all_labels.extend(labels.numpy())\n",
                "\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(all_labels, all_preds, target_names=list(ACCENT_LABELS.values())))\n",
                "\n",
                "cm = confusion_matrix(all_labels, all_preds)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=ACCENT_LABELS.values(), yticklabels=ACCENT_LABELS.values())\n",
                "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
                "plt.title('Accent Detection Confusion Matrix')\n",
                "plt.savefig('accent_confusion_matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Save & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "checkpoint = {\n",
                "    'model_state_dict': model.state_dict(),\n",
                "    'accent_labels': ACCENT_LABELS,\n",
                "    'num_classes': NUM_CLASSES,\n",
                "    'sample_rate': SAMPLE_RATE,\n",
                "    'n_mels': N_MELS,\n",
                "    'n_fft': N_FFT,\n",
                "    'hop_length': HOP_LENGTH,\n",
                "    'best_val_acc': best_val_acc\n",
                "}\n",
                "torch.save(checkpoint, 'accent_model_full.pth')\n",
                "print(\"‚úÖ Saved: accent_model.pth, accent_model_full.pth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "files.download('accent_model.pth')\n",
                "files.download('accent_model_full.pth')\n",
                "files.download('accent_training_curves.png')\n",
                "files.download('accent_confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Next Steps\\n\",\n",
                "\n",
                "1. Download `accent_model.pth`\n",
                "2. Create `AccentModel/` folder in project\n",
                "3. Integrate with backend\n",
                "4. Test accent detection"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
