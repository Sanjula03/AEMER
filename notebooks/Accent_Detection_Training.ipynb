{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ AEMER - Accent Detection Model Training (16 Accents)\n",
    "\n",
    "**Architecture:** CNN-BiLSTM with Attention\n",
    "\n",
    "**Dataset:** Mozilla Common Voice (English) ‚Äî Real speech data\n",
    "\n",
    "**Output Classes:** 16 English accents worldwide\n",
    "\n",
    "**Author:** Sanjula Sunath | w1999522\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install torch torchaudio librosa pandas numpy scikit-learn tqdm matplotlib seaborn datasets --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Speech Accent Archive from Kaggle\n",
    "!pip install kagglehub --quiet\n",
    "import kagglehub\n",
    "\n",
    "print(\"üì• Downloading Speech Accent Archive from Kaggle...\")\n",
    "dataset_path = kagglehub.dataset_download(\"rtatman/speech-accent-archive\")\n",
    "print(f\"‚úÖ Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "import os\n",
    "print(\"\\nFiles:\")\n",
    "for f in os.listdir(dataset_path):\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Check for recordings directory\n",
    "recordings_dir = None\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for d in dirs:\n",
    "        if 'recording' in d.lower():\n",
    "            recordings_dir = os.path.join(root, d)\n",
    "            break\n",
    "    if recordings_dir:\n",
    "        break\n",
    "\n",
    "# Also check if MP3s are directly in dataset_path\n",
    "if recordings_dir is None:\n",
    "    mp3_files = [f for f in os.listdir(dataset_path) if f.endswith('.mp3')]\n",
    "    if mp3_files:\n",
    "        recordings_dir = dataset_path\n",
    "\n",
    "print(f\"üìÇ Recordings directory: {recordings_dir}\")\n",
    "if recordings_dir:\n",
    "    audio_files = [f for f in os.listdir(recordings_dir) if f.endswith('.mp3') or f.endswith('.wav')]\n",
    "    print(f\"   Found {len(audio_files)} audio files\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build accent dataset from Speech Accent Archive metadata\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Find the CSV metadata file\n",
    "csv_files = glob.glob(os.path.join(dataset_path, '**', '*.csv'), recursive=True)\n",
    "print(f\"Found CSV files: {csv_files}\")\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(csv_files[0])\n",
    "print(f\"\\nMetadata shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nNative languages (top 20):\")\n",
    "print(df['native_language'].value_counts().head(20))\n",
    "\n",
    "# Map native languages to our accent classes\n",
    "LANGUAGE_TO_ACCENT = {\n",
    "    # American English\n",
    "    'english': None,  # Will split by country/birth_place later\n",
    "    \n",
    "    # South Asian  \n",
    "    'hindi': 'indian', 'urdu': 'indian', 'bengali': 'indian',\n",
    "    'tamil': 'indian', 'telugu': 'indian', 'gujarati': 'indian',\n",
    "    'punjabi': 'indian', 'marathi': 'indian', 'kannada': 'indian',\n",
    "    'malayalam': 'indian', 'nepali': 'indian', 'sinhala': 'indian',\n",
    "    'sinhalese': 'indian', 'dari': 'indian',\n",
    "    \n",
    "    # East Asian\n",
    "    'mandarin': 'hongkong', 'cantonese': 'hongkong',\n",
    "    'chinese': 'hongkong',\n",
    "    \n",
    "    # Southeast Asian\n",
    "    'malay': 'malaysian', 'bahasa': 'malaysian',\n",
    "    'tagalog': 'filipino', 'cebuano': 'filipino', 'ilocano': 'filipino',\n",
    "    \n",
    "    # African accents\n",
    "    'amharic': 'african', 'swahili': 'african', 'yoruba': 'african',\n",
    "    'igbo': 'african', 'hausa': 'african', 'zulu': 'african',\n",
    "    'twi': 'african', 'shona': 'african', 'akan': 'african',\n",
    "    'luganda': 'african', 'wolof': 'african', 'somali': 'african',\n",
    "    'kinyarwanda': 'african', 'xhosa': 'african', 'sesotho': 'african',\n",
    "    'afrikaans': 'african',\n",
    "\n",
    "    # Irish\n",
    "    'irish': 'irish', 'gaelic': 'irish',\n",
    "\n",
    "    # Scottish  \n",
    "    'scots': 'scottish',\n",
    "\n",
    "    # Welsh\n",
    "    'welsh': 'welsh',\n",
    "    \n",
    "    # Korean\n",
    "    'korean': 'singaporean',  # Group with SE Asian\n",
    "    \n",
    "    # Japanese\n",
    "    'japanese': 'hongkong',  # Group with East Asian\n",
    "    \n",
    "    # Arabic\n",
    "    'arabic': 'african',\n",
    "    \n",
    "    # Spanish-influenced\n",
    "    'spanish': 'bermudian',  # Map to available class\n",
    "    \n",
    "    # French\n",
    "    'french': 'bermudian',\n",
    "    \n",
    "    # German  \n",
    "    'german': 'southatlantic',\n",
    "    \n",
    "    # Portuguese\n",
    "    'portuguese': 'southatlantic',\n",
    "    \n",
    "    # Dutch\n",
    "    'dutch': 'southatlantic',\n",
    "    \n",
    "    # Russian / Eastern European\n",
    "    'russian': 'southatlantic',\n",
    "    'polish': 'southatlantic',\n",
    "    'turkish': 'southatlantic',\n",
    "    'romanian': 'southatlantic',\n",
    "    'czech': 'southatlantic',\n",
    "    'hungarian': 'southatlantic',\n",
    "    'serbian': 'southatlantic',\n",
    "    'croatian': 'southatlantic',\n",
    "    'bulgarian': 'southatlantic',\n",
    "    'ukrainian': 'southatlantic',\n",
    "    \n",
    "    # Italian\n",
    "    'italian': 'bermudian',\n",
    "    \n",
    "    # Vietnamese\n",
    "    'vietnamese': 'singaporean',\n",
    "    'thai': 'singaporean',\n",
    "    'indonesian': 'singaporean',\n",
    "}\n",
    "\n",
    "# Countries to help classify native English speakers\n",
    "COUNTRY_TO_ACCENT = {\n",
    "    'usa': 'american', 'united states': 'american', 'us': 'american',\n",
    "    'uk': 'british', 'england': 'british', 'united kingdom': 'british',\n",
    "    'australia': 'australian',\n",
    "    'canada': 'canadian',\n",
    "    'new zealand': 'newzealand',\n",
    "    'scotland': 'scottish',\n",
    "    'ireland': 'irish',\n",
    "    'wales': 'welsh',\n",
    "    'south africa': 'african',\n",
    "    'nigeria': 'african', 'kenya': 'african', 'ghana': 'african',\n",
    "    'india': 'indian', 'pakistan': 'indian', 'bangladesh': 'indian', 'sri lanka': 'indian',\n",
    "    'malaysia': 'malaysian',\n",
    "    'philippines': 'filipino',\n",
    "    'singapore': 'singaporean',\n",
    "    'hong kong': 'hongkong',\n",
    "    'bermuda': 'bermudian',\n",
    "}\n",
    "\n",
    "data_list = []\n",
    "accent_counts = {}\n",
    "skipped = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        lang = str(row.get('native_language', '')).strip().lower()\n",
    "        country = str(row.get('country', '')).strip().lower()\n",
    "        birth_place = str(row.get('birthplace', '')).strip().lower()\n",
    "        filename = str(row.get('filename', ''))\n",
    "        \n",
    "        if not filename or filename == 'nan':\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Determine accent\n",
    "        accent = None\n",
    "        \n",
    "        # First try language mapping\n",
    "        if lang in LANGUAGE_TO_ACCENT:\n",
    "            accent = LANGUAGE_TO_ACCENT[lang]\n",
    "        \n",
    "        # For native English speakers, use country\n",
    "        if accent is None and lang == 'english':\n",
    "            for key, acc in COUNTRY_TO_ACCENT.items():\n",
    "                if key in country or key in birth_place:\n",
    "                    accent = acc\n",
    "                    break\n",
    "            if accent is None:\n",
    "                accent = 'american'  # Default English to American\n",
    "        \n",
    "        # Try country mapping if still None\n",
    "        if accent is None:\n",
    "            for key, acc in COUNTRY_TO_ACCENT.items():\n",
    "                if key in country or key in birth_place:\n",
    "                    accent = acc\n",
    "                    break\n",
    "        \n",
    "        if accent is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        if accent not in ACCENT_TO_IDX:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        label = ACCENT_TO_IDX[accent]\n",
    "        \n",
    "        # Find audio file\n",
    "        audio_path = None\n",
    "        if recordings_dir:\n",
    "            # Try common filename patterns\n",
    "            for ext in ['.mp3', '.wav']:\n",
    "                candidate = os.path.join(recordings_dir, filename + ext)\n",
    "                if os.path.exists(candidate):\n",
    "                    audio_path = candidate\n",
    "                    break\n",
    "                # Also try without extension if filename already has it\n",
    "                candidate = os.path.join(recordings_dir, filename)\n",
    "                if os.path.exists(candidate):\n",
    "                    audio_path = candidate\n",
    "                    break\n",
    "        \n",
    "        if audio_path is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        data_list.append({\n",
    "            'audio_path': audio_path,\n",
    "            'label': label,\n",
    "            'accent': accent,\n",
    "        })\n",
    "        accent_counts[accent] = accent_counts.get(accent, 0) + 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Processed: {len(data_list)} samples\")\n",
    "print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "print(f\"\\nüìä Samples per accent:\")\n",
    "for accent, count in sorted(accent_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {accent}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance & filter accents\n",
    "from collections import defaultdict\n",
    "\n",
    "# Only keep accents with enough samples\n",
    "MIN_SAMPLES = 10  # Lower threshold since dataset is smaller\n",
    "samples_by_class = defaultdict(list)\n",
    "for d in data_list:\n",
    "    samples_by_class[d['label']].append(d)\n",
    "\n",
    "# Filter out classes with too few samples  \n",
    "valid_classes = {label for label, samples in samples_by_class.items() if len(samples) >= MIN_SAMPLES}\n",
    "\n",
    "print(f\"Accents with >= {MIN_SAMPLES} samples:\")\n",
    "for label in sorted(valid_classes):\n",
    "    accent = ACCENT_LABELS[label]\n",
    "    count = len(samples_by_class[label])\n",
    "    print(f\"  [{label}] {accent}: {count} samples\")\n",
    "\n",
    "# Rebuild with only valid classes, remap labels\n",
    "valid_data = [d for d in data_list if d['label'] in valid_classes]\n",
    "sorted_valid = sorted(valid_classes)\n",
    "old_to_new = {old: new for new, old in enumerate(sorted_valid)}\n",
    "\n",
    "FINAL_ACCENTS = [ACCENT_LABELS[old] for old in sorted_valid]\n",
    "FINAL_ACCENT_TO_IDX = {a: i for i, a in enumerate(FINAL_ACCENTS)}\n",
    "FINAL_IDX_TO_ACCENT = {i: a for i, a in enumerate(FINAL_ACCENTS)}\n",
    "FINAL_NUM_CLASSES = len(FINAL_ACCENTS)\n",
    "\n",
    "for d in valid_data:\n",
    "    d['label'] = old_to_new[d['label']]\n",
    "\n",
    "# Balance using oversampling\n",
    "target_size = int(np.median([len(samples_by_class[c]) for c in valid_classes]))\n",
    "target_size = max(target_size, MIN_SAMPLES * 2)\n",
    "\n",
    "balanced_data = []\n",
    "new_samples_by_class = defaultdict(list)\n",
    "for d in valid_data:\n",
    "    new_samples_by_class[d['label']].append(d)\n",
    "\n",
    "for label, samples in new_samples_by_class.items():\n",
    "    if len(samples) >= target_size:\n",
    "        balanced_data.extend(random.sample(samples, target_size))\n",
    "    else:\n",
    "        balanced_data.extend(samples)\n",
    "        extra = target_size - len(samples)\n",
    "        balanced_data.extend(random.choices(samples, k=extra))\n",
    "\n",
    "random.shuffle(balanced_data)\n",
    "data_list = balanced_data\n",
    "\n",
    "print(f\"\\n‚úÖ Final: {FINAL_NUM_CLASSES} accent classes, {len(data_list)} total samples\")\n",
    "print(f\"\\nüìä Balanced distribution:\")\n",
    "from collections import Counter\n",
    "final_counts = Counter(d['label'] for d in data_list)\n",
    "for label in sorted(final_counts.keys()):\n",
    "    print(f\"  [{label}] {FINAL_IDX_TO_ACCENT[label]}: {final_counts[label]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, data_list, augment=False):\n",
    "        self.data = data_list\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_audio(self, audio_path):\n",
    "        # Load audio file\n",
    "        audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE, duration=DURATION + 1)\n",
    "        \n",
    "        # Trim silence\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "\n",
    "        # Normalize\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "\n",
    "        # Fix length\n",
    "        target = int(DURATION * SAMPLE_RATE)\n",
    "        if len(audio) < target:\n",
    "            audio = np.pad(audio, (0, target - len(audio)))\n",
    "        else:\n",
    "            if self.augment and len(audio) > target:\n",
    "                start = random.randint(0, len(audio) - target)\n",
    "                audio = audio[start:start + target]\n",
    "            else:\n",
    "                audio = audio[:target]\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            if random.random() < 0.3:\n",
    "                audio = audio + np.random.randn(len(audio)) * 0.005\n",
    "            if random.random() < 0.2:\n",
    "                n_steps = random.uniform(-1, 1)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=SAMPLE_RATE, n_steps=n_steps)\n",
    "            if random.random() < 0.2:\n",
    "                rate = random.uniform(0.9, 1.1)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "                if len(audio) < target:\n",
    "                    audio = np.pad(audio, (0, target - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target]\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=SAMPLE_RATE, n_mels=N_MELS,\n",
    "            n_fft=N_FFT, hop_length=HOP_LENGTH\n",
    "        )\n",
    "        spec = librosa.power_to_db(mel, ref=np.max)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-8)\n",
    "\n",
    "        if spec.shape[1] < MAX_LEN:\n",
    "            spec = np.pad(spec, ((0, 0), (0, MAX_LEN - spec.shape[1])))\n",
    "        else:\n",
    "            spec = spec[:, :MAX_LEN]\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        label = item['label']\n",
    "        try:\n",
    "            spec = self.process_audio(item['audio_path'])\n",
    "        except Exception as e:\n",
    "            # Return zeros on error (rare)\n",
    "            spec = np.zeros((N_MELS, MAX_LEN))\n",
    "        return torch.FloatTensor(spec).unsqueeze(0), label\n",
    "\n",
    "print(\"‚úÖ AccentDataset class defined (loads from audio files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build Accent Dataset from Common Voice\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process Common Voice data and filter by accent\n",
    "import soundfile as sf\n",
    "from collections import Counter\n",
    "\n",
    "data_list = []\n",
    "accent_counts = Counter()\n",
    "skipped = 0\n",
    "\n",
    "print(\"üîÑ Processing audio files and mapping accents...\")\n",
    "\n",
    "for i, sample in enumerate(tqdm(cv_dataset, desc=\"Processing\")):\n",
    "    try:\n",
    "        # Get accent label from the sample\n",
    "        accent_raw = sample.get('accent', '') or ''\n",
    "        accent_raw = accent_raw.strip().lower()\n",
    "        \n",
    "        if not accent_raw:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Map to our accent classes\n",
    "        mapped_accent = CV_ACCENT_MAP.get(accent_raw)\n",
    "        if mapped_accent is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        label = ACCENT_TO_IDX[mapped_accent]\n",
    "        \n",
    "        # Get audio\n",
    "        audio_info = sample.get('audio', None)\n",
    "        if audio_info is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        audio_array = audio_info['array']\n",
    "        sr = audio_info['sampling_rate']\n",
    "        \n",
    "        if len(audio_array) < sr * 0.5:  # Skip very short clips (<0.5s)\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        data_list.append({\n",
    "            'audio': audio_array,\n",
    "            'sample_rate': sr,\n",
    "            'label': label,\n",
    "        })\n",
    "        accent_counts[mapped_accent] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Processed samples: {len(data_list)}\")\n",
    "print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "print(f\"\\nüìä Samples per accent:\")\n",
    "for accent, count in sorted(accent_counts.items(), key=lambda x: -x[1]):\n",
    "    flag = '‚úÖ' if count >= MIN_SAMPLES_PER_CLASS else '‚ö†Ô∏è'\n",
    "    print(f\"  {flag} {accent}: {count}\")\n",
    "\n",
    "# Remove accents with too few samples\n",
    "valid_accents = {a for a, c in accent_counts.items() if c >= MIN_SAMPLES_PER_CLASS}\n",
    "data_list = [d for d in data_list if ACCENT_LABELS[d['label']] in valid_accents]\n",
    "\n",
    "print(f\"\\nüéØ Using {len(valid_accents)} accents with >= {MIN_SAMPLES_PER_CLASS} samples\")\n",
    "print(f\"üìä Total training samples: {len(data_list)}\")\n",
    "\n",
    "# Rebuild label mapping for valid accents only\n",
    "FINAL_ACCENTS = sorted(valid_accents)\n",
    "FINAL_ACCENT_TO_IDX = {a: i for i, a in enumerate(FINAL_ACCENTS)}\n",
    "FINAL_IDX_TO_ACCENT = {i: a for a, i in FINAL_ACCENT_TO_IDX.items()}\n",
    "FINAL_NUM_CLASSES = len(FINAL_ACCENTS)\n",
    "\n",
    "# Remap labels\n",
    "for d in data_list:\n",
    "    old_accent = ACCENT_LABELS[d['label']]\n",
    "    d['label'] = FINAL_ACCENT_TO_IDX[old_accent]\n",
    "\n",
    "print(f\"\\n‚úÖ Final accent classes ({FINAL_NUM_CLASSES}):\")\n",
    "for i, accent in FINAL_IDX_TO_ACCENT.items():\n",
    "    count = sum(1 for d in data_list if d['label'] == i)\n",
    "    print(f\"  [{i}] {accent}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Balance Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Balance using oversampling (duplicate minority class samples)\n",
    "from collections import defaultdict\n",
    "\n",
    "samples_by_class = defaultdict(list)\n",
    "for d in data_list:\n",
    "    samples_by_class[d['label']].append(d)\n",
    "\n",
    "# Target: match the median class size (avoid extreme over/under sampling)\n",
    "class_sizes = [len(v) for v in samples_by_class.values()]\n",
    "target_size = int(np.median(class_sizes))\n",
    "target_size = min(target_size, max(class_sizes))  # Don't exceed max\n",
    "\n",
    "print(f\"‚öñÔ∏è Balancing to ~{target_size} samples per class\")\n",
    "\n",
    "balanced_data = []\n",
    "for label, samples in samples_by_class.items():\n",
    "    if len(samples) >= target_size:\n",
    "        # Downsample: random selection\n",
    "        balanced_data.extend(random.sample(samples, target_size))\n",
    "    else:\n",
    "        # Oversample: repeat + random extra\n",
    "        balanced_data.extend(samples)\n",
    "        extra_needed = target_size - len(samples)\n",
    "        balanced_data.extend(random.choices(samples, k=extra_needed))\n",
    "\n",
    "random.shuffle(balanced_data)\n",
    "data_list = balanced_data\n",
    "\n",
    "print(f\"‚úÖ Balanced dataset: {len(data_list)} total samples\")\n",
    "print(f\"üìä Per class:\")\n",
    "final_counts = Counter(d['label'] for d in data_list)\n",
    "for label in sorted(final_counts.keys()):\n",
    "    print(f\"  [{label}] {FINAL_IDX_TO_ACCENT[label]}: {final_counts[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, data_list, augment=False):\n",
    "        self.data = data_list\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_audio(self, audio, sr):\n",
    "        # Resample to target rate\n",
    "        if sr != SAMPLE_RATE:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "\n",
    "        # Trim silence\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "\n",
    "        # Normalize\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "\n",
    "        # Fix length to DURATION seconds\n",
    "        target = int(DURATION * SAMPLE_RATE)\n",
    "        if len(audio) < target:\n",
    "            audio = np.pad(audio, (0, target - len(audio)))\n",
    "        else:\n",
    "            # Random crop for augmentation, center crop otherwise\n",
    "            if self.augment and len(audio) > target:\n",
    "                start = random.randint(0, len(audio) - target)\n",
    "                audio = audio[start:start + target]\n",
    "            else:\n",
    "                audio = audio[:target]\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            # Random noise\n",
    "            if random.random() < 0.3:\n",
    "                noise = np.random.randn(len(audio)) * 0.005\n",
    "                audio = audio + noise\n",
    "            # Random pitch shift\n",
    "            if random.random() < 0.2:\n",
    "                n_steps = random.uniform(-1, 1)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=SAMPLE_RATE, n_steps=n_steps)\n",
    "            # Random speed change\n",
    "            if random.random() < 0.2:\n",
    "                rate = random.uniform(0.9, 1.1)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "                if len(audio) < target:\n",
    "                    audio = np.pad(audio, (0, target - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target]\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=SAMPLE_RATE, n_mels=N_MELS,\n",
    "            n_fft=N_FFT, hop_length=HOP_LENGTH\n",
    "        )\n",
    "        spec = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "        # Normalize spectrogram\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-8)\n",
    "\n",
    "        # Fix time dimension\n",
    "        if spec.shape[1] < MAX_LEN:\n",
    "            spec = np.pad(spec, ((0, 0), (0, MAX_LEN - spec.shape[1])))\n",
    "        else:\n",
    "            spec = spec[:, :MAX_LEN]\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        label = item['label']\n",
    "        audio = np.array(item['audio'], dtype=np.float32)\n",
    "        sr = item['sample_rate']\n",
    "        spec = self.process_audio(audio, sr)\n",
    "        return torch.FloatTensor(spec).unsqueeze(0), label\n",
    "\n",
    "print(\"‚úÖ AccentDataset class defined (with augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ CNN-BiLSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNN_BiLSTM_Accent(nn.Module):\n",
    "    def __init__(self, num_classes=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.3))\n",
    "        self.lstm = nn.LSTM(128 * 16, 128, 2, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.attention = nn.Sequential(nn.Linear(256, 64), nn.Tanh(), nn.Linear(64, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.conv2(self.conv1(x)))\n",
    "        x = x.permute(0, 3, 1, 2).reshape(x.size(0), -1, 128 * 16)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        return self.fc(torch.sum(attn * lstm_out, dim=1))\n",
    "\n",
    "model = CNN_BiLSTM_Accent(FINAL_NUM_CLASSES).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"‚úÖ Model created for {FINAL_NUM_CLASSES} accent classes\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Prepare Data Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data, val_data = train_test_split(\n",
    "    data_list, test_size=0.2, random_state=42,\n",
    "    stratify=[d['label'] for d in data_list]\n",
    ")\n",
    "print(f\"Training: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "\n",
    "train_dataset = AccentDataset(train_data, augment=True)\n",
    "val_dataset = AccentDataset(val_data, augment=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Training with Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Class weights for imbalanced data\n",
    "from collections import Counter\n",
    "label_counts = Counter(d['label'] for d in train_data)\n",
    "total = sum(label_counts.values())\n",
    "class_weights = torch.FloatTensor([total / (FINAL_NUM_CLASSES * label_counts.get(i, 1)) for i in range(FINAL_NUM_CLASSES)]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "PATIENCE = 8\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(f\"Starting training for up to {EPOCHS} epochs (patience={PATIENCE})...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, pred = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history['train_loss'].append(train_loss / len(train_loader))\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss / len(val_loader))\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Early stopping on val_loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'accent_model.pth')\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} ‚≠ê BEST (loss: {val_loss/len(val_loader):.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} (no improvement {patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nüõë Early stopping at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üéâ Training complete! Best Val Loss: {best_val_loss/len(val_loader):.4f} | Best Val Acc: {best_val_acc:.4f}\")\n",
    "print(f\"   Total epochs: {len(history['train_loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Visualize Training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss'); axes[0].set_xlabel('Epoch'); axes[0].legend(); axes[0].grid(True)\n",
    "axes[1].plot([a*100 for a in history['train_acc']], label='Train')\n",
    "axes[1].plot([a*100 for a in history['val_acc']], label='Val')\n",
    "axes[1].set_title('Accuracy (%)'); axes[1].set_xlabel('Epoch'); axes[1].legend(); axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('accent_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluation & Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('accent_model.pth'))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs.to(device))\n",
    "        _, pred = outputs.max(1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accent_names = [FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)]\n",
    "\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=accent_names))\n",
    "\n",
    "# Confusion matrices (normalized + raw)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1%', cmap='Blues',\n",
    "            xticklabels=accent_names, yticklabels=accent_names, ax=axes[0],\n",
    "            vmin=0, vmax=1)\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Normalized Confusion Matrix (Recall)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Raw\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=accent_names, yticklabels=accent_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Raw Counts')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accent_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüéØ Per-class accuracy (recall):\")\n",
    "for i, name in enumerate(accent_names):\n",
    "    recall = cm_normalized[i, i] if i < len(cm_normalized) else 0\n",
    "    print(f\"  {name}: {recall:.1%}\")\n",
    "print(f\"  Overall: {np.trace(cm)/cm.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Save & Download\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save model with metadata\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'accent_labels': FINAL_IDX_TO_ACCENT,\n",
    "    'num_classes': FINAL_NUM_CLASSES,\n",
    "    'accents_list': [FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)],\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'n_mels': N_MELS,\n",
    "    'n_fft': N_FFT,\n",
    "    'hop_length': HOP_LENGTH,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'accent_model_full.pth')\n",
    "\n",
    "model_size = os.path.getsize('accent_model.pth') / (1024 * 1024)\n",
    "print(f\"‚úÖ accent_model.pth ({model_size:.1f} MB)\")\n",
    "print(f\"‚úÖ accent_model_full.pth (with metadata)\")\n",
    "print(f\"\\nüéØ Accent classes ({FINAL_NUM_CLASSES}):\")\n",
    "for i in range(FINAL_NUM_CLASSES):\n",
    "    print(f\"  [{i}] {FINAL_IDX_TO_ACCENT[i]}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è IMPORTANT: Update backend ACCENTS list to match this order!\")\n",
    "print(f\"Copy this to app.py:\")\n",
    "print(f\"ACCENTS = {[FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import files\n",
    "files.download('accent_model.pth')\n",
    "files.download('accent_model_full.pth')\n",
    "files.download('accent_training_curves.png')\n",
    "files.download('accent_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Next Steps\n",
    "\n",
    "1. Download `accent_model.pth`\n",
    "2. Place in `AccentModel/` folder\n",
    "3. Update `ACCENTS` list in `app.py` and `video_handler.py`\n",
    "4. Update `ResultModal.tsx` accent flags in frontend\n",
    "5. Deploy to HF Space\n",
    "6. Test all accent predictions\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}