{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ AEMER - Accent Detection Model Training (16 Accents)\n",
    "\n",
    "**Architecture:** CNN-BiLSTM with Attention\n",
    "\n",
    "**Dataset:** Mozilla Common Voice (English) ‚Äî Real speech data\n",
    "\n",
    "**Output Classes:** 16 English accents worldwide\n",
    "\n",
    "**Author:** Sanjula Sunath | w1999522\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install torch torchaudio librosa pandas numpy scikit-learn tqdm matplotlib seaborn datasets --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration ‚Äî 16 Accent Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === 16 English Accent Classes ===\n",
    "ACCENT_LABELS = {\n",
    "    0: 'american',       # üá∫üá∏ United States\n",
    "    1: 'british',        # üá¨üáß England\n",
    "    2: 'australian',     # üá¶üá∫ Australia\n",
    "    3: 'indian',         # üáÆüá≥ India / South Asia\n",
    "    4: 'canadian',       # üá®üá¶ Canada\n",
    "    5: 'scottish',       # üè¥ Scotland\n",
    "    6: 'irish',          # üáÆüá™ Ireland\n",
    "    7: 'african',        # üåç Africa\n",
    "    8: 'newzealand',     # üá≥üáø New Zealand\n",
    "    9: 'welsh',          # üè¥ Wales\n",
    "    10: 'malaysian',     # üá≤üáæ Malaysia\n",
    "    11: 'filipino',      # üáµüá≠ Philippines\n",
    "    12: 'singaporean',   # üá∏üá¨ Singapore\n",
    "    13: 'hongkong',      # üá≠üá∞ Hong Kong\n",
    "    14: 'bermudian',     # üáßüá≤ Bermuda\n",
    "    15: 'southatlantic', # üåä South Atlantic\n",
    "}\n",
    "\n",
    "# Reverse mapping: accent string ‚Üí index\n",
    "ACCENT_TO_IDX = {v: k for k, v in ACCENT_LABELS.items()}\n",
    "NUM_CLASSES = 16\n",
    "\n",
    "# Common Voice accent field ‚Üí our label mapping\n",
    "CV_ACCENT_MAP = {\n",
    "    'us': 'american',\n",
    "    'united states english': 'american',\n",
    "    'england': 'british',\n",
    "    'british english': 'british',\n",
    "    'australia': 'australian',\n",
    "    'australian english': 'australian',\n",
    "    'indian': 'indian',\n",
    "    'india and south asia': 'indian',\n",
    "    'canada': 'canadian',\n",
    "    'canadian english': 'canadian',\n",
    "    'scotland': 'scottish',\n",
    "    'scottish english': 'scottish',\n",
    "    'ireland': 'irish',\n",
    "    'irish english': 'irish',\n",
    "    'african': 'african',\n",
    "    'african english': 'african',\n",
    "    'newzealand': 'newzealand',\n",
    "    'new zealand english': 'newzealand',\n",
    "    'wales': 'welsh',\n",
    "    'welsh english': 'welsh',\n",
    "    'malaysia': 'malaysian',\n",
    "    'malaysian english': 'malaysian',\n",
    "    'philippines': 'filipino',\n",
    "    'filipino': 'filipino',\n",
    "    'singapore': 'singaporean',\n",
    "    'singaporean english': 'singaporean',\n",
    "    'hongkong': 'hongkong',\n",
    "    'hong kong english': 'hongkong',\n",
    "    'bermuda': 'bermudian',\n",
    "    'bermudian english': 'bermudian',\n",
    "    'southatlantic': 'southatlantic',\n",
    "    'south atlantic': 'southatlantic',\n",
    "}\n",
    "\n",
    "# Audio parameters\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 3\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 160\n",
    "MAX_LEN = int(SAMPLE_RATE * DURATION / HOP_LENGTH) + 1\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "MIN_SAMPLES_PER_CLASS = 50  # Minimum samples needed per accent\n",
    "\n",
    "print(f\"üéØ Target: {NUM_CLASSES} accent classes\")\n",
    "print(f\"üìä Audio: {SAMPLE_RATE}Hz, {DURATION}s, {N_MELS} mel bins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download Mozilla Common Voice Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download Common Voice English dataset using HuggingFace datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"üì• Loading Mozilla Common Voice English dataset...\")\n",
    "print(\"   This may take a while on first run (~5GB download)\")\n",
    "\n",
    "# Load the English subset with accent metadata\n",
    "# Using streaming to avoid downloading everything at once\n",
    "try:\n",
    "    cv_dataset = load_dataset(\n",
    "        \"mozilla-foundation/common_voice_17_0\",\n",
    "        \"en\",\n",
    "        split=\"train\",\n",
    "        trust_remote_code=True,\n",
    "        token=True  # You may need to accept terms on HF\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded {len(cv_dataset)} total samples\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load Common Voice directly: {e}\")\n",
    "    print(\"\\nTrying alternative: CommonAccent dataset...\")\n",
    "    # Fallback: Use the CommonAccent pre-filtered dataset\n",
    "    cv_dataset = load_dataset(\n",
    "        \"Jzuluaga/accent-id-commonaccent_en\",\n",
    "        split=\"train\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded CommonAccent dataset: {len(cv_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build Accent Dataset from Common Voice\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process Common Voice data and filter by accent\n",
    "import soundfile as sf\n",
    "from collections import Counter\n",
    "\n",
    "data_list = []\n",
    "accent_counts = Counter()\n",
    "skipped = 0\n",
    "\n",
    "print(\"üîÑ Processing audio files and mapping accents...\")\n",
    "\n",
    "for i, sample in enumerate(tqdm(cv_dataset, desc=\"Processing\")):\n",
    "    try:\n",
    "        # Get accent label from the sample\n",
    "        accent_raw = sample.get('accent', '') or ''\n",
    "        accent_raw = accent_raw.strip().lower()\n",
    "        \n",
    "        if not accent_raw:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Map to our accent classes\n",
    "        mapped_accent = CV_ACCENT_MAP.get(accent_raw)\n",
    "        if mapped_accent is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        label = ACCENT_TO_IDX[mapped_accent]\n",
    "        \n",
    "        # Get audio\n",
    "        audio_info = sample.get('audio', None)\n",
    "        if audio_info is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        audio_array = audio_info['array']\n",
    "        sr = audio_info['sampling_rate']\n",
    "        \n",
    "        if len(audio_array) < sr * 0.5:  # Skip very short clips (<0.5s)\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        data_list.append({\n",
    "            'audio': audio_array,\n",
    "            'sample_rate': sr,\n",
    "            'label': label,\n",
    "        })\n",
    "        accent_counts[mapped_accent] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Processed samples: {len(data_list)}\")\n",
    "print(f\"‚è≠Ô∏è Skipped: {skipped}\")\n",
    "print(f\"\\nüìä Samples per accent:\")\n",
    "for accent, count in sorted(accent_counts.items(), key=lambda x: -x[1]):\n",
    "    flag = '‚úÖ' if count >= MIN_SAMPLES_PER_CLASS else '‚ö†Ô∏è'\n",
    "    print(f\"  {flag} {accent}: {count}\")\n",
    "\n",
    "# Remove accents with too few samples\n",
    "valid_accents = {a for a, c in accent_counts.items() if c >= MIN_SAMPLES_PER_CLASS}\n",
    "data_list = [d for d in data_list if ACCENT_LABELS[d['label']] in valid_accents]\n",
    "\n",
    "print(f\"\\nüéØ Using {len(valid_accents)} accents with >= {MIN_SAMPLES_PER_CLASS} samples\")\n",
    "print(f\"üìä Total training samples: {len(data_list)}\")\n",
    "\n",
    "# Rebuild label mapping for valid accents only\n",
    "FINAL_ACCENTS = sorted(valid_accents)\n",
    "FINAL_ACCENT_TO_IDX = {a: i for i, a in enumerate(FINAL_ACCENTS)}\n",
    "FINAL_IDX_TO_ACCENT = {i: a for a, i in FINAL_ACCENT_TO_IDX.items()}\n",
    "FINAL_NUM_CLASSES = len(FINAL_ACCENTS)\n",
    "\n",
    "# Remap labels\n",
    "for d in data_list:\n",
    "    old_accent = ACCENT_LABELS[d['label']]\n",
    "    d['label'] = FINAL_ACCENT_TO_IDX[old_accent]\n",
    "\n",
    "print(f\"\\n‚úÖ Final accent classes ({FINAL_NUM_CLASSES}):\")\n",
    "for i, accent in FINAL_IDX_TO_ACCENT.items():\n",
    "    count = sum(1 for d in data_list if d['label'] == i)\n",
    "    print(f\"  [{i}] {accent}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Balance Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Balance using oversampling (duplicate minority class samples)\n",
    "from collections import defaultdict\n",
    "\n",
    "samples_by_class = defaultdict(list)\n",
    "for d in data_list:\n",
    "    samples_by_class[d['label']].append(d)\n",
    "\n",
    "# Target: match the median class size (avoid extreme over/under sampling)\n",
    "class_sizes = [len(v) for v in samples_by_class.values()]\n",
    "target_size = int(np.median(class_sizes))\n",
    "target_size = min(target_size, max(class_sizes))  # Don't exceed max\n",
    "\n",
    "print(f\"‚öñÔ∏è Balancing to ~{target_size} samples per class\")\n",
    "\n",
    "balanced_data = []\n",
    "for label, samples in samples_by_class.items():\n",
    "    if len(samples) >= target_size:\n",
    "        # Downsample: random selection\n",
    "        balanced_data.extend(random.sample(samples, target_size))\n",
    "    else:\n",
    "        # Oversample: repeat + random extra\n",
    "        balanced_data.extend(samples)\n",
    "        extra_needed = target_size - len(samples)\n",
    "        balanced_data.extend(random.choices(samples, k=extra_needed))\n",
    "\n",
    "random.shuffle(balanced_data)\n",
    "data_list = balanced_data\n",
    "\n",
    "print(f\"‚úÖ Balanced dataset: {len(data_list)} total samples\")\n",
    "print(f\"üìä Per class:\")\n",
    "final_counts = Counter(d['label'] for d in data_list)\n",
    "for label in sorted(final_counts.keys()):\n",
    "    print(f\"  [{label}] {FINAL_IDX_TO_ACCENT[label]}: {final_counts[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, data_list, augment=False):\n",
    "        self.data = data_list\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_audio(self, audio, sr):\n",
    "        # Resample to target rate\n",
    "        if sr != SAMPLE_RATE:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=SAMPLE_RATE)\n",
    "\n",
    "        # Trim silence\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "\n",
    "        # Normalize\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "\n",
    "        # Fix length to DURATION seconds\n",
    "        target = int(DURATION * SAMPLE_RATE)\n",
    "        if len(audio) < target:\n",
    "            audio = np.pad(audio, (0, target - len(audio)))\n",
    "        else:\n",
    "            # Random crop for augmentation, center crop otherwise\n",
    "            if self.augment and len(audio) > target:\n",
    "                start = random.randint(0, len(audio) - target)\n",
    "                audio = audio[start:start + target]\n",
    "            else:\n",
    "                audio = audio[:target]\n",
    "\n",
    "        # Data augmentation\n",
    "        if self.augment:\n",
    "            # Random noise\n",
    "            if random.random() < 0.3:\n",
    "                noise = np.random.randn(len(audio)) * 0.005\n",
    "                audio = audio + noise\n",
    "            # Random pitch shift\n",
    "            if random.random() < 0.2:\n",
    "                n_steps = random.uniform(-1, 1)\n",
    "                audio = librosa.effects.pitch_shift(audio, sr=SAMPLE_RATE, n_steps=n_steps)\n",
    "            # Random speed change\n",
    "            if random.random() < 0.2:\n",
    "                rate = random.uniform(0.9, 1.1)\n",
    "                audio = librosa.effects.time_stretch(audio, rate=rate)\n",
    "                if len(audio) < target:\n",
    "                    audio = np.pad(audio, (0, target - len(audio)))\n",
    "                else:\n",
    "                    audio = audio[:target]\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=SAMPLE_RATE, n_mels=N_MELS,\n",
    "            n_fft=N_FFT, hop_length=HOP_LENGTH\n",
    "        )\n",
    "        spec = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "        # Normalize spectrogram\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-8)\n",
    "\n",
    "        # Fix time dimension\n",
    "        if spec.shape[1] < MAX_LEN:\n",
    "            spec = np.pad(spec, ((0, 0), (0, MAX_LEN - spec.shape[1])))\n",
    "        else:\n",
    "            spec = spec[:, :MAX_LEN]\n",
    "\n",
    "        return spec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        label = item['label']\n",
    "        audio = np.array(item['audio'], dtype=np.float32)\n",
    "        sr = item['sample_rate']\n",
    "        spec = self.process_audio(audio, sr)\n",
    "        return torch.FloatTensor(spec).unsqueeze(0), label\n",
    "\n",
    "print(\"‚úÖ AccentDataset class defined (with augmentation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ CNN-BiLSTM Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNN_BiLSTM_Accent(nn.Module):\n",
    "    def __init__(self, num_classes=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2), nn.Dropout(0.3))\n",
    "        self.lstm = nn.LSTM(128 * 16, 128, 2, batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.attention = nn.Sequential(nn.Linear(256, 64), nn.Tanh(), nn.Linear(64, 1))\n",
    "        self.fc = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.4), nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.conv2(self.conv1(x)))\n",
    "        x = x.permute(0, 3, 1, 2).reshape(x.size(0), -1, 128 * 16)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        attn = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        return self.fc(torch.sum(attn * lstm_out, dim=1))\n",
    "\n",
    "model = CNN_BiLSTM_Accent(FINAL_NUM_CLASSES).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"‚úÖ Model created for {FINAL_NUM_CLASSES} accent classes\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Prepare Data Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_data, val_data = train_test_split(\n",
    "    data_list, test_size=0.2, random_state=42,\n",
    "    stratify=[d['label'] for d in data_list]\n",
    ")\n",
    "print(f\"Training: {len(train_data)}, Validation: {len(val_data)}\")\n",
    "\n",
    "train_dataset = AccentDataset(train_data, augment=True)\n",
    "val_dataset = AccentDataset(val_data, augment=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Training with Early Stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Class weights for imbalanced data\n",
    "from collections import Counter\n",
    "label_counts = Counter(d['label'] for d in train_data)\n",
    "total = sum(label_counts.values())\n",
    "class_weights = torch.FloatTensor([total / (FINAL_NUM_CLASSES * label_counts.get(i, 1)) for i in range(FINAL_NUM_CLASSES)]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "PATIENCE = 8\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(f\"Starting training for up to {EPOCHS} epochs (patience={PATIENCE})...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, pred = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history['train_loss'].append(train_loss / len(train_loader))\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss / len(val_loader))\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Early stopping on val_loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'accent_model.pth')\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} ‚≠ê BEST (loss: {val_loss/len(val_loader):.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} (no improvement {patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nüõë Early stopping at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"üéâ Training complete! Best Val Loss: {best_val_loss/len(val_loader):.4f} | Best Val Acc: {best_val_acc:.4f}\")\n",
    "print(f\"   Total epochs: {len(history['train_loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Visualize Training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_title('Loss'); axes[0].set_xlabel('Epoch'); axes[0].legend(); axes[0].grid(True)\n",
    "axes[1].plot([a*100 for a in history['train_acc']], label='Train')\n",
    "axes[1].plot([a*100 for a in history['val_acc']], label='Val')\n",
    "axes[1].set_title('Accuracy (%)'); axes[1].set_xlabel('Epoch'); axes[1].legend(); axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('accent_training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluation & Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('accent_model.pth'))\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs.to(device))\n",
    "        _, pred = outputs.max(1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accent_names = [FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)]\n",
    "\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=accent_names))\n",
    "\n",
    "# Confusion matrices (normalized + raw)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1%', cmap='Blues',\n",
    "            xticklabels=accent_names, yticklabels=accent_names, ax=axes[0],\n",
    "            vmin=0, vmax=1)\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Normalized Confusion Matrix (Recall)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Raw\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=accent_names, yticklabels=accent_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Raw Counts')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accent_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüéØ Per-class accuracy (recall):\")\n",
    "for i, name in enumerate(accent_names):\n",
    "    recall = cm_normalized[i, i] if i < len(cm_normalized) else 0\n",
    "    print(f\"  {name}: {recall:.1%}\")\n",
    "print(f\"  Overall: {np.trace(cm)/cm.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Save & Download\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save model with metadata\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'accent_labels': FINAL_IDX_TO_ACCENT,\n",
    "    'num_classes': FINAL_NUM_CLASSES,\n",
    "    'accents_list': [FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)],\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'n_mels': N_MELS,\n",
    "    'n_fft': N_FFT,\n",
    "    'hop_length': HOP_LENGTH,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'accent_model_full.pth')\n",
    "\n",
    "model_size = os.path.getsize('accent_model.pth') / (1024 * 1024)\n",
    "print(f\"‚úÖ accent_model.pth ({model_size:.1f} MB)\")\n",
    "print(f\"‚úÖ accent_model_full.pth (with metadata)\")\n",
    "print(f\"\\nüéØ Accent classes ({FINAL_NUM_CLASSES}):\")\n",
    "for i in range(FINAL_NUM_CLASSES):\n",
    "    print(f\"  [{i}] {FINAL_IDX_TO_ACCENT[i]}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è IMPORTANT: Update backend ACCENTS list to match this order!\")\n",
    "print(f\"Copy this to app.py:\")\n",
    "print(f\"ACCENTS = {[FINAL_IDX_TO_ACCENT[i] for i in range(FINAL_NUM_CLASSES)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import files\n",
    "files.download('accent_model.pth')\n",
    "files.download('accent_model_full.pth')\n",
    "files.download('accent_training_curves.png')\n",
    "files.download('accent_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Next Steps\n",
    "\n",
    "1. Download `accent_model.pth`\n",
    "2. Place in `AccentModel/` folder\n",
    "3. Update `ACCENTS` list in `app.py` and `video_handler.py`\n",
    "4. Update `ResultModal.tsx` accent flags in frontend\n",
    "5. Deploy to HF Space\n",
    "6. Test all accent predictions\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}