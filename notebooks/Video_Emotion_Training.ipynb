{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Emotion Model Training (ResNet-18 Transfer Learning)\n",
    "\n",
    "Train a facial emotion recognition model using transfer learning.\n",
    "\n",
    "**Dataset**: FER2013 (35K grayscale face images, 48x48)\n",
    "\n",
    "**Model**: ResNet-18 pretrained → fine-tuned for 4 emotions\n",
    "\n",
    "**Expected Accuracy**: 75-80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install kaggle --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download FER2013 Dataset\n",
    "\n",
    "**Option A**: Upload `kaggle.json` (Kaggle API key)\n",
    "\n",
    "**Option B**: Manually download from https://www.kaggle.com/datasets/msambare/fer2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Use Kaggle API (upload kaggle.json first)\n",
    "# from google.colab import files\n",
    "# files.upload()  # Upload kaggle.json\n",
    "\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !mv kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d msambare/fer2013\n",
    "# !unzip -q fer2013.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Upload manually from Google Drive or local\n",
    "# Assumes you've uploaded and unzipped the dataset\n",
    "# Expected structure:\n",
    "# ./train/angry/, ./train/happy/, ./train/sad/, ./train/neutral/, etc.\n",
    "# ./test/angry/, ./test/happy/, ./test/sad/, ./test/neutral/, etc.\n",
    "\n",
    "TRAIN_DIR = './train'\n",
    "TEST_DIR = './test'\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    print(\"Dataset found!\")\n",
    "    for emotion in os.listdir(TRAIN_DIR):\n",
    "        count = len(os.listdir(os.path.join(TRAIN_DIR, emotion)))\n",
    "        print(f\"  {emotion}: {count} images\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please upload FER2013 dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Emotion Mapping\n",
    "\n",
    "FER2013 has 7 emotions → Map to AEMER's 4 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FER2013 original emotions: angry, disgust, fear, happy, sad, surprise, neutral\n",
    "# AEMER target emotions: angry, happy, sad, neutral\n",
    "\n",
    "# Mapping: disgust→angry, fear→sad, surprise→happy\n",
    "EMOTION_MAPPING = {\n",
    "    'angry': 'angry',\n",
    "    'disgust': 'angry',     # Map to angry\n",
    "    'fear': 'sad',          # Map to sad\n",
    "    'happy': 'happy',\n",
    "    'sad': 'sad',\n",
    "    'surprise': 'happy',    # Map to happy (positive emotion)\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "TARGET_EMOTIONS = ['angry', 'happy', 'sad', 'neutral']\n",
    "EMOTION_TO_IDX = {e: i for i, e in enumerate(TARGET_EMOTIONS)}\n",
    "IDX_TO_EMOTION = {i: e for e, i in EMOTION_TO_IDX.items()}\n",
    "\n",
    "print(\"Emotion mapping:\")\n",
    "print(EMOTION_TO_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER2013Dataset(Dataset):\n",
    "    \"\"\"FER2013 Dataset with emotion mapping to 4 classes.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load all images\n",
    "        for emotion_folder in os.listdir(root_dir):\n",
    "            emotion_path = os.path.join(root_dir, emotion_folder)\n",
    "            if not os.path.isdir(emotion_path):\n",
    "                continue\n",
    "            \n",
    "            # Map to target emotion\n",
    "            mapped_emotion = EMOTION_MAPPING.get(emotion_folder.lower())\n",
    "            if mapped_emotion is None:\n",
    "                continue\n",
    "            \n",
    "            label = EMOTION_TO_IDX[mapped_emotion]\n",
    "            \n",
    "            for img_file in os.listdir(emotion_path):\n",
    "                img_path = os.path.join(emotion_path, img_file)\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images\")\n",
    "        \n",
    "        # Print class distribution\n",
    "        from collections import Counter\n",
    "        dist = Counter(self.labels)\n",
    "        for idx, count in sorted(dist.items()):\n",
    "            print(f\"  {IDX_TO_EMOTION[idx]}: {count}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert grayscale to RGB for ResNet\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transforms & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet expects 224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = FER2013Dataset(TRAIN_DIR, transform=train_transform)\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_dataset = FER2013Dataset(TEST_DIR, transform=val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Model (ResNet-18 Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialEmotionResNet(nn.Module):\n",
    "    \"\"\"ResNet-18 based facial emotion classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-18\n",
    "        self.resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Freeze early layers (keep last 2 blocks + FC trainable)\n",
    "        ct = 0\n",
    "        for child in self.resnet.children():\n",
    "            ct += 1\n",
    "            if ct < 7:  # Freeze first 6 of 8 children (conv1, bn1, relu, maxpool, layer1, layer2)\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Replace final FC layer with deeper head\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = FacialEmotionResNet(num_classes=4, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
    "print(f\"Frozen: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function with class weights (handle imbalance)\n",
    "from collections import Counter\n",
    "label_counts = Counter(train_dataset.labels)\n",
    "total = sum(label_counts.values())\n",
    "class_weights = torch.FloatTensor([total / label_counts[i] for i in range(4)]).to(device)\n",
    "class_weights = class_weights / class_weights.sum() * 4  # Normalize\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer with different learning rates\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.resnet.fc.parameters(), 'lr': 1e-3},       # New layers - higher LR\n",
    "    {'params': list(model.resnet.parameters())[:-2], 'lr': 1e-4}  # Pretrained - lower LR\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with early stopping\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 4\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(f\"Training for up to {NUM_EPOCHS} epochs (early stopping patience={PATIENCE})...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Early stopping based on val_loss (lower is better)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'video_model.pth')\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% \\u2b50 BEST\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% (no improvement {patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\n\\U0001f6d1 Early stopping at epoch {epoch+1}!\")\n",
    "            break\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Val Loss: {best_val_loss:.4f} | Best Val Acc: {best_val_acc:.2f}%\")\n",
    "print(f\"Total epochs run: {len(history['train_loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['val_loss'], label='Validation')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'], label='Validation')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('video_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=TARGET_EMOTIONS))\n",
    "\n",
    "# Confusion matrices (normalized + raw)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Normalized (percentages)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=TARGET_EMOTIONS, yticklabels=TARGET_EMOTIONS, ax=axes[0],\n",
    "            vmin=0, vmax=1)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Normalized Confusion Matrix (Recall per Class)')\n",
    "\n",
    "# Right: Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=TARGET_EMOTIONS, yticklabels=TARGET_EMOTIONS, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Raw Counts)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('video_confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy (recall):\")\n",
    "for i, name in enumerate(TARGET_EMOTIONS):\n",
    "    recall = cm_normalized[i, i]\n",
    "    print(f\"  {name}: {recall:.1%}\")\n",
    "print(f\"  Overall: {np.trace(cm)/cm.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test on Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, image_path):\n",
    "    \"\"\"Predict emotion from a single image.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
    "        predicted_idx = probabilities.argmax().item()\n",
    "        confidence = probabilities[predicted_idx].item()\n",
    "    \n",
    "    predicted_emotion = IDX_TO_EMOTION[predicted_idx]\n",
    "    \n",
    "    print(f\"Predicted: {predicted_emotion} ({confidence:.2%})\")\n",
    "    print(\"All probabilities:\")\n",
    "    for i, emotion in enumerate(TARGET_EMOTIONS):\n",
    "        print(f\"  {emotion}: {probabilities[i].item():.2%}\")\n",
    "    \n",
    "    return predicted_emotion, confidence\n",
    "\n",
    "# Test on a sample image from test set\n",
    "sample_img = test_dataset.images[0]\n",
    "print(f\"Testing on: {sample_img}\")\n",
    "predict_emotion(model, sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model size\n",
    "model_size = os.path.getsize('video_model.pth') / (1024 * 1024)\n",
    "print(f\"Model size: {model_size:.2f} MB\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('video_model.pth')\n",
    "print(\"\\n✅ Download video_model.pth and place in project/VideoModel/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Model**: ResNet-18 Transfer Learning\n",
    "\n",
    "**Input**: 224x224 RGB image (face crop)\n",
    "\n",
    "**Output**: 4 emotions (angry, happy, sad, neutral)\n",
    "\n",
    "**Usage**:\n",
    "1. Detect face in video frame (OpenCV)\n",
    "2. Crop & resize to 224x224\n",
    "3. Normalize with ImageNet stats\n",
    "4. Predict with model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}